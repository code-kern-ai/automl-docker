{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-12 14:35:39.022098: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-05-12 14:35:39.022136: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Filter all debugging messages from tensorflow \n",
    "#from silence_tensorflow import silence_tensorflow\n",
    "#silence_tensorflow()\n",
    "\n",
    "# Logistic Regression spits out warnings on large datasets. For now, there warning will be surpressed.\n",
    "# This warning will get taken care of in a later version\n",
    "#import warnings\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Basis libraries\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Sklearn libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix, mean_squared_error\n",
    "\n",
    "# Embedders and Transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Tensorflow \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_getter(path_statement):\n",
    "    while True:\n",
    "        user_input = input()\n",
    "        print(path_statement, PATH)\n",
    "        path_approval = input('(y/ n) ')\n",
    "        if path_approval.lower() == 'y':\n",
    "            break\n",
    "        elif path_approval.lower() == 'n':\n",
    "            print('>> Enter a new path: ')\n",
    "        else:\n",
    "            print(\">> Sorry, that didn't work. Please enter again: \")\n",
    "    return user_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Please select the path, where your data is stored!\n",
      ">> On Windows the path might look like this: C:\\Users\\yourname\\data\\training_data.csv\n",
      ">> On MacOS/ Linux the path might look like this: home/user/data/training_data.csv\n",
      " \n",
      ">> Is this the correct path? -> clickbait_data.csv\n",
      " \n",
      ">> Please provide the column name in which the texts are store in!\n",
      ">> Is this the correct column name? -> headline\n",
      ">> Data successfully loaded!\n",
      " \n",
      ">> Please provide the column name in which the labels are store in!\n",
      ">> Is this the correct column name? -> clickbait\n"
     ]
    }
   ],
   "source": [
    "# get datetime dd/mm/YY H:M\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime('%d-%m-%Y %H-%M')\n",
    "\n",
    "# Read in the data with pandas, then convert text corpus to list\n",
    "print('>> Please select the path, where your data is stored!')\n",
    "print('>> On Windows the path might look like this: C:\\\\Users\\\\yourname\\\\data\\\\training_data.csv')\n",
    "print('>> On MacOS/ Linux the path might look like this: home/user/data/training_data.csv')\n",
    "print(' ')\n",
    "\n",
    "# Get the path where the data is stored\n",
    "PATH = input_getter('>> Is this the correct path? ->')\n",
    "\n",
    "# Get the name of the features\n",
    "print(' ')\n",
    "print('>> Please provide the column name in which the texts are store in!')\n",
    "COL_TEXTS = input_getter('>> Is this the correct column name? ->')\n",
    "\n",
    "# Load the data with the provided info\n",
    "df = pd.read_csv(PATH)\n",
    "corpus = df[COL_TEXTS].to_list()\n",
    "print('>> Data successfully loaded!')\n",
    "\n",
    "# Lowering all words\n",
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()\n",
    "\n",
    "# Get the names of the labels\n",
    "COL_LABEL = input_getter('>> Is this the correct column name? ->')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ">> Please input a number to choose your method of embedding.\n",
      ">> 1 - Transformer based embeddings (accurate, but slower)\n",
      ">> 2 - TF-IDF Vectorizer (faster, but less accurate)\n",
      " \n",
      ">> Creating TF-IDF embeddings ...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    print(' ')\n",
    "    print('>> Please input a number to choose your method of embedding.')\n",
    "    print('>> 1 - Transformer based embeddings (accurate, but slower)')\n",
    "    print('>> 2 - TF-IDF Vectorizer (faster, but less accurate)')\n",
    "    print(' ')\n",
    "\n",
    "    choice = input()\n",
    "\n",
    "    if choice == '1':\n",
    "        # Instantiate a sentence transformer and create embeddings \n",
    "        print('>> Creating embeddings using transformer model, this might take a couple of minutes ...')\n",
    "        sent_transformer = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "        embeddings = sent_transformer.encode(corpus)\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        # Intantiate a tf-idf vectorizer\n",
    "        print('>> Creating TF-IDF embeddings ...')\n",
    "        vect = TfidfVectorizer()\n",
    "        embeddings = vect.fit_transform(corpus)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set embedding as features\n",
    "features = embeddings\n",
    "\n",
    "labels = df[COL_LABEL]\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ">> Please input a number to choose your algorithm:\n",
      ">> 1 - Logistic Regression\n",
      ">> 2 - Random Forest Classifier\n",
      ">> 3 - XGBoost Classifier\n",
      ">> 4 - Noise Robust Deep Neural Network (work in progress)\n",
      "Trainign neural network ...\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leopuettmann/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/sequential_32/dense_122/embedding_lookup_sparse/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/sequential_32/dense_122/embedding_lookup_sparse/Reshape:0\", shape=(None, 64), dtype=float32), dense_shape=Tensor(\"gradient_tape/sequential_32/dense_122/embedding_lookup_sparse/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 7s 8ms/step - loss: 0.1339 - accuracy: 0.9539\n",
      "Epoch 2/10\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.0207 - accuracy: 0.9932\n",
      "Epoch 3/10\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.0041 - accuracy: 0.9988\n",
      "Epoch 4/10\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.0011 - accuracy: 0.9998\n",
      "Epoch 5/10\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 2.2174e-04 - accuracy: 0.9999\n",
      "Epoch 6/10\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 7.8724e-05 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 6.1774e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 1.0868e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 9.9878e-05 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Selecting a model\n",
    "while True:\n",
    "    print(' ')\n",
    "    print('>> Please input a number to choose your algorithm:')\n",
    "    print('>> 1 - Logistic Regression')\n",
    "    print('>> 2 - Random Forest Classifier')\n",
    "    print('>> 3 - XGBoost Classifier')\n",
    "    print('>> 4 - Noise Robust Deep Neural Network (work in progress)')\n",
    "\n",
    "    model_choice = input()\n",
    "\n",
    "    if model_choice == '1':\n",
    "        print('>> Training a logistic regression ...')\n",
    "        lr = LogisticRegression(dual=False)\n",
    "\n",
    "        # Hyper parameter space is relatively small\n",
    "        hyperparameters = {'C': np.arange(0, 4), \n",
    "                        'penalty': ['l2', 'None'],\n",
    "                        'max_iter': [100, 150, 250, 500]}\n",
    "\n",
    "        # Initiate and fit random search cv\n",
    "        lr_clf = RandomizedSearchCV(estimator=lr, param_distributions=hyperparameters, cv=5,  n_iter=5)\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "        # Save the model to current directory\n",
    "        with open(f'Logistic Regression {dt_string}.pkl', 'wb') as fid:\n",
    "            pickle.dump(lr_clf, fid) \n",
    "            print(f'>> Saved model to {os.path.abspath(os.getcwd())}') \n",
    "        break\n",
    "\n",
    "    elif model_choice == '2':\n",
    "        print('Training a random forest ...')\n",
    "        rf = RandomForestClassifier()\n",
    "\n",
    "        # Hyper parameter space is relatively small\n",
    "        hyperparameters = {'max_depth': [90, 100, 110], \n",
    "                        'min_samples_leaf': [3, 4, 5]}\n",
    "\n",
    "        # Initiate and fit random search cv\n",
    "        rf_clf = RandomizedSearchCV(estimator=rf, param_distributions=hyperparameters, cv=3, n_iter=3)\n",
    "        rf_clf.fit(X_train, y_train)\n",
    "        y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "        # Save the model to current directory\n",
    "        with open(f'Random Forest {dt_string}.pkl', 'wb') as fid:\n",
    "            pickle.dump(rf_clf, fid) \n",
    "            print(f'>> Saved model to {os.path.abspath(os.getcwd())}') \n",
    "        break\n",
    "\n",
    "    elif model_choice == '3':\n",
    "        print('Training a XGBoost classifier ...')\n",
    "        xg_cl = xgb.XGBClassifier(eval_metric='auc')\n",
    "\n",
    "        # Hyperparameter space is relatively narrow\n",
    "        hyperparameters = {'eta': np.arange(0.1, 0.5, 0.1), \n",
    "                        'max_depth': [4, 6, 8, 10], \n",
    "                        'n_estimators': np.arange(10, 400, 20), }\n",
    "\n",
    "        # Initiate and fit random search cv\n",
    "        xgb_clf = RandomizedSearchCV(estimator=xg_cl, param_distributions=hyperparameters, cv=3, n_iter=3)\n",
    "        xgb_clf.fit(X_train, y_train)\n",
    "        y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "        # Save the model to current directory\n",
    "        with open(f'XGBoost {dt_string}.pkl', 'wb') as fid:\n",
    "            pickle.dump(xgb_clf, fid) \n",
    "            print(f'>> Saved model to {os.path.abspath(os.getcwd())}') \n",
    "        break\n",
    "\n",
    "    elif model_choice == '4':\n",
    "        print('Trainign neural network ...')\n",
    "        X_train.astype('float16')\n",
    "        X_test.astype('float16')\n",
    "        y_train.astype('float16')\n",
    "\n",
    "        # Vectorize labels\n",
    "        y_train = np.asarray(y_train).astype('float32').reshape((-1,1))\n",
    "        y_test = np.asarray(y_test).astype('float32').reshape((-1,1))\n",
    "\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "\n",
    "        neural_net = Sequential()\n",
    "        \n",
    "        neural_net.add(Dense(64, activation='relu', input_shape=(X_train.shape[1], )))\n",
    "        neural_net.add(Dropout(0.2))\n",
    "        neural_net.add(Dense(64, activation='relu'))\n",
    "        neural_net.add(Dropout(0.2))\n",
    "        neural_net.add(Dense(32, activation='relu'))\n",
    "        neural_net.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        neural_net.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        neural_net.fit(X_train, y_train, epochs=5, verbose=True, callbacks=[callback])\n",
    "\n",
    "        y_pred = (neural_net.predict(X_test) > 0.5).astype(\"int32\")\n",
    "        break\n",
    "\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      ">> Generating evaluation metrics ...\n",
      " \n",
      "- - - - - - - - - - - - - - - -\n",
      ">> Model accuracy is: 98.0 %\n",
      "- - - - - - - - - - - - - - - -\n",
      ">> Mean squared error is: 0.16\n",
      "- - - - - - - - - - - - - - - -\n",
      ">> AUC is: 0.98\n",
      "- - - - - - - - - - - - - - - -\n",
      ">> The confusion matrix is: [[3109   79]\n",
      " [  81 3131]]\n"
     ]
    }
   ],
   "source": [
    "print(' ')\n",
    "print('>> Generating evaluation metrics ...')\n",
    "print(' ')\n",
    "print('- - - - - - - - - - - - - - - -')\n",
    "print(f'>> Model accuracy is: {round(accuracy_score(y_test, y_pred), 2) * 100} %')\n",
    "print('- - - - - - - - - - - - - - - -')\n",
    "print(f'>> Mean squared error is: {round(np.sqrt(mean_squared_error(y_test, y_pred)), 2)}')\n",
    "print('- - - - - - - - - - - - - - - -')\n",
    "print(f'>> AUC is: {round(roc_auc_score(y_test, y_pred), 2)}')\n",
    "print('- - - - - - - - - - - - - - - -')\n",
    "print(f'>> The confusion matrix is: {confusion_matrix(y_test, y_pred)}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
